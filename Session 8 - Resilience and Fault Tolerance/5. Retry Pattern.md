# Retry Pattern

**Categorization: Proactive**
The Retry Pattern is categorized as proactive because it involves taking action before a failure has a significant impact. The goal is to handle transient faults by automatically retrying a failed operation, based on a predefined policy. This approach anticipates that some failures might be temporary and can be resolved by retrying the operation.

**How It Works:**
The Retry Pattern involves configuring a set of rules for retrying an operation when it fails. These rules typically include:
- **Retry Count:** The number of times the operation should be retried before giving up.
- **Backoff Strategy:** The delay between retries, which could be fixed or incremental. This helps prevent overwhelming the service with retries in rapid succession.
- **Retryable Exceptions:** The types of exceptions that trigger a retry. Typically, these are transient errors, such as network timeouts or temporary unavailability of a service.

When an operation fails and triggers an exception that matches the retryable criteria, the system waits for the specified backoff duration and then retries the operation. This process continues until either the operation succeeds or the maximum retry count is reached.

**Sample Application in Real Life:**
Consider a microservice that calls an external API for data retrieval. If the external API experiences a temporary issue or network glitch, the Retry Pattern can be employed to retry the request a few times before reporting a failure. This ensures that temporary issues do not lead to immediate failure in the microservice, improving overall resilience.

**Parameters Required to Make It Work:**
- **Retry Count:** Number of retry attempts to make before giving up.
- **Backoff Strategy:** Defines how the retry delay is calculated, such as fixed delay, exponential backoff, etc.
- **Exception Types:** A list of exceptions that should trigger a retry.
- **Timeouts:** Optionally, timeouts can be set for how long to wait for each retry and how long to wait in total before considering the retry attempts exhausted.

In summary, the Retry Pattern is a proactive strategy designed to handle transient errors by automatically retrying operations based on configurable rules, enhancing the resilience and fault tolerance of microservices.

## Sample Code Usage

### 1. Microservice with Spring Cloud LoadBalancer and Feign Client

**Dependencies:**

Ensure the following dependencies are included in your `pom.xml`:

```xml
<dependencies>
    <!-- Spring Cloud Circuit Breaker with Resilience4j -->
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-circuitbreaker-reactor-resilience4j</artifactId>
    </dependency>
    <!-- Spring Cloud LoadBalancer -->
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-loadbalancer</artifactId>
    </dependency>
    <!-- Spring Cloud OpenFeign -->
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-openfeign</artifactId>
    </dependency>
</dependencies>
```

**Feign Client Configuration:**

Define a Feign client to interact with another microservice:

```java
@FeignClient(name = "product-service", configuration = ProductServiceClientConfig.class)
public interface ProductServiceClient {
    @GetMapping("/products/{id}")
    Mono<Product> getProductById(@PathVariable("id") String id);
}
```

**Retry Configuration:**

Create a configuration class to customize the retry mechanism for the Feign client:

```java
@Configuration
public class ProductServiceClientConfig {
    
    @Bean
    public ReactiveRetryPolicy retryPolicy() {
        return Retry.fixedDelay(3, Duration.ofSeconds(2))
                    .filter(throwable -> throwable instanceof TimeoutException || throwable instanceof IOException);
    }
    
    @Bean
    public ReactiveResilience4JCircuitBreakerFactory circuitBreakerFactory(ReactiveRetryPolicy retryPolicy) {
        ReactiveResilience4JCircuitBreakerFactory factory = new ReactiveResilience4JCircuitBreakerFactory();
        factory.configure(builder -> builder.retryPolicy(retryPolicy), "product-service");
        return factory;
    }
}
```

**Service Layer:**

Apply the retry mechanism to a method in your service layer:

```java
@Service
public class ProductService {
    
    private final ProductServiceClient productServiceClient;
    private final ReactiveCircuitBreakerFactory<?, ?> circuitBreakerFactory;

    @Autowired
    public ProductService(ProductServiceClient productServiceClient,
                          ReactiveCircuitBreakerFactory<?, ?> circuitBreakerFactory) {
        this.productServiceClient = productServiceClient;
        this.circuitBreakerFactory = circuitBreakerFactory;
    }

    public Mono<Product> getProductById(String id) {
        return circuitBreakerFactory.create("product-service")
                                    .run(productServiceClient.getProductById(id), throwable -> Mono.just(new Product("default-id", "default-name")));
    }
}
```

**Explanation:**

- **Feign Client:** The `ProductServiceClient` interface is a Feign client that connects to the `product-service` microservice.
- **Retry Configuration:** The `ProductServiceClientConfig` class defines a retry policy that retries a maximum of 3 times with a 2-second delay for specific exceptions. The retry configuration is set up using the `ReactiveResilience4JCircuitBreakerFactory`.
- **Service Layer:** The `ProductService` class uses the `circuitBreakerFactory` to apply the retry logic. If the call fails, it defaults to a fallback response.

### 2. API Gateway with Spring Cloud Gateway

**Dependencies:**

Include the necessary dependencies in your `pom.xml`:

```xml
<dependencies>
    <!-- Spring Cloud Gateway -->
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-gateway</artifactId>
    </dependency>
    <!-- Spring Cloud Circuit Breaker with Resilience4j -->
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-circuitbreaker-reactor-resilience4j</artifactId>
    </dependency>
    <!-- Spring Cloud LoadBalancer -->
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-loadbalancer</artifactId>
    </dependency>
</dependencies>
```

**YAML Configuration:**

Define the gateway routes and retry configuration in `application.yml`:

```yaml
spring:
  cloud:
    gateway:
      routes:
        - id: product-service
          uri: lb://PRODUCT-SERVICE
          predicates:
            - Path=/products/**
          filters:
            - name: CircuitBreaker
              args:
                name: productServiceCircuitBreaker
                fallbackUri: forward:/fallback
            - name: Retry
              args:
                retries: 3
                statuses: BAD_GATEWAY, GATEWAY_TIMEOUT, INTERNAL_SERVER_ERROR
                backoff:
                  firstBackoff: 2s
                  maxBackoff: 10s
                  factor: 2
    circuitbreaker:
      reactive-resilience4j:
        productServiceCircuitBreaker:
          slidingWindowSize: 10
          failureRateThreshold: 50
          waitDurationInOpenState: 5000
          permittedNumberOfCallsInHalfOpenState: 5
          minimumNumberOfCalls: 5
```

**Fallback Controller:**

Create a fallback controller to handle fallback responses:

```java
@RestController
public class FallbackController {

    @RequestMapping("/fallback")
    public Mono<String> fallback() {
        return Mono.just("Product Service is currently unavailable. Please try again later.");
    }
}
```

**Explanation:**

- **YAML Configuration:** Defines a route for the `product-service` with a circuit breaker and retry filter. The retry filter retries 3 times for specific HTTP statuses with exponential backoff. The circuit breaker configuration specifies a sliding window size, failure rate threshold, and other parameters.
- **Fallback Controller:** Handles fallback responses when the circuit breaker is open, providing a default message to the user.